{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Simon Swenson\n",
    "# Collaborated with: Subin Sapkota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn import tree as tr\n",
    "\n",
    "data = pd.read_csv(\"iris1.csv\").values\n",
    "\n",
    "X = data[:, :4]\n",
    "y = data[:, 4]\n",
    "\n",
    "accuracy = {'entropy': 0.0, 'gini': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of entropy: 0.7916666666666667\n",
      "Accuracy of gini: 0.825\n",
      "Maximum accuracy hyperparameter was gini: 0.825\n",
      "Outer accuracy of gini was 0.875\n",
      "Accuracy of entropy: 0.825\n",
      "Accuracy of gini: 0.85\n",
      "Maximum accuracy hyperparameter was gini: 0.85\n",
      "Outer accuracy of gini was 0.9583333333333334\n",
      "Accuracy of entropy: 0.7583333333333333\n",
      "Accuracy of gini: 0.825\n",
      "Maximum accuracy hyperparameter was gini: 0.825\n",
      "Outer accuracy of gini was 0.875\n",
      "Accuracy of entropy: 0.7916666666666666\n",
      "Accuracy of gini: 0.7666666666666666\n",
      "Maximum accuracy hyperparameter was entropy: 0.7916666666666666\n",
      "Outer accuracy of entropy was 0.7916666666666666\n",
      "Accuracy of entropy: 0.8\n",
      "Accuracy of gini: 0.8166666666666667\n",
      "Maximum accuracy hyperparameter was gini: 0.8166666666666667\n",
      "Outer accuracy of gini was 0.9166666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "num_folds = 5\n",
    "\n",
    "# Just doing a shuffled k-fold might be sufficient, but that's random. Stratification ensures\n",
    "# an equal distribution of the labels.\n",
    "k_folds = ms.StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = 0)\n",
    "# kFolds = ms.KFold(n_splits = 5, shuffle = True)\n",
    "for train_indeces_i, test_indeces_i in k_folds.split(X, y):\n",
    "    # Get the current k-fold data points\n",
    "    X_train_i = X[train_indeces_i]\n",
    "    X_test_i = X[test_indeces_i]\n",
    "    y_train_i = y[train_indeces_i]\n",
    "    y_test_i = y[test_indeces_i]\n",
    "    \n",
    "    accuracy = {'entropy': 0.0, 'gini': 0.0}\n",
    "    \n",
    "    # Split the test data into inner folds. Same num splits.\n",
    "    # Already shuffled, so don't bother again.\n",
    "    k_folds = ms.StratifiedKFold(n_splits = num_folds)\n",
    "    for train_indeces_j, test_indeces_j in k_folds.split(X_train_i, y_train_i):\n",
    "        X_train_j = X_train_i[train_indeces_j]\n",
    "        X_test_j = X_train_i[test_indeces_j]\n",
    "        y_train_j = y_train_i[train_indeces_j]\n",
    "        y_test_j = y_train_i[test_indeces_j]\n",
    "        \n",
    "        # Begin classifier pipeline\n",
    "    \n",
    "        # Scale and norm the input data\n",
    "        standard_scalar = pp.StandardScaler()\n",
    "        normalizer = pp.Normalizer(norm = 'l1')\n",
    "        X_train_j = normalizer.fit_transform(standard_scalar.fit_transform(X_train_j))\n",
    "        X_test_j = normalizer.transform(standard_scalar.transform(X_test_j))\n",
    "    \n",
    "        # Encode the labels\n",
    "        le = pp.LabelEncoder()\n",
    "        y_train_j = le.fit_transform(y_train_j)\n",
    "        y_test_j = le.transform(y_test_j)\n",
    "        \n",
    "        # Train/Test model with different hyperparameters. Made this extensible with a\n",
    "        # loop for some reason. Didn't want to have to basically write out the same code twice.\n",
    "        for key in accuracy:\n",
    "            tree = tr.DecisionTreeClassifier(criterion = key)\n",
    "            tree.fit(X_train_j, y_train_j)\n",
    "            X_test_j_results = tree.predict(X_test_j)\n",
    "            # Compute the accuracy by summing all instances where the classifier's output\n",
    "            # matches the actual class, then dividing by the number of rows.\n",
    "            accuracy[key] += float(np.sum([\n",
    "                1 if xcr_cur == yte_cur else 0 \\\n",
    "                for xcr_cur, yte_cur \\\n",
    "                in zip(X_test_j_results, y_test_j)])) \\\n",
    "                / len(y_test_j)\n",
    "                \n",
    "        # End classifier pipeline\n",
    "        \n",
    "    # Normalize the accuracy, since we had 5 folds [0, 5] -> [0, 1]\n",
    "    max_accuracy = 0.0\n",
    "    max_accuracy_criterion = ''\n",
    "    for key in accuracy:\n",
    "        accuracy[key] /= num_folds\n",
    "        print('Accuracy of ' + key + \": \" + str(accuracy[key]))\n",
    "        if accuracy[key] > max_accuracy:\n",
    "            max_accuracy = accuracy[key]\n",
    "            max_accuracy_criterion = key\n",
    "            \n",
    "    print('Maximum accuracy hyperparameter was ' + max_accuracy_criterion + ': ' + str(max_accuracy))\n",
    "\n",
    "    # Now that we know which has the highest accuracy, let's use that for the outer\n",
    "    # train/test data.\n",
    "    \n",
    "    # Begin classifier pipeline\n",
    "    \n",
    "    # Scale and norm the input data\n",
    "    standard_scalar = pp.StandardScaler()\n",
    "    normalizer = pp.Normalizer(norm = 'l1')\n",
    "    X_train_i = normalizer.fit_transform(standard_scalar.fit_transform(X_train_i))\n",
    "    X_test_i = normalizer.transform(standard_scalar.transform(X_test_i))\n",
    "    \n",
    "    # Encode the labels\n",
    "    le = pp.LabelEncoder()\n",
    "    y_train_i = le.fit_transform(y_train_i)\n",
    "    y_test_i = le.transform(y_test_i)  \n",
    "    \n",
    "    # Train/Test the tree\n",
    "    tree = tr.DecisionTreeClassifier(criterion = max_accuracy_criterion)\n",
    "    tree.fit(X_train_i, y_train_i)\n",
    "    X_test_i_results = tree.predict(X_test_i)\n",
    "    \n",
    "    # Compute the accuracy by summing all instances where the classifier's output\n",
    "    # matches the actual class, then dividing by the number of rows.\n",
    "    accuracy = float(np.sum([\n",
    "        1 if xcr_cur == yte_cur else 0 \\\n",
    "        for xcr_cur, yte_cur \\\n",
    "        in zip(X_test_j_results, y_test_j)])) \\\n",
    "        / len(y_test_j)\n",
    "        \n",
    "    print('Outer accuracy of ' + max_accuracy_criterion + ' was ' + str(accuracy))\n",
    "    \n",
    "    # End classifier pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
