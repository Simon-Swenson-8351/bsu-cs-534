{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS 534\n",
    "\n",
    "Problem Set 2\n",
    "\n",
    "Simon Swenson (ID: 113072053)\n",
    "\n",
    "Collaborated with: Subin Sapkota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Set 2 (25 pts)\n",
    "\n",
    "Assume the following dataset:\n",
    "\n",
    "|  x1 | x2 | x3 |  x4 | y(Class) |\n",
    "| ---:| --:| --:| ---:| --------:|\n",
    "|   2 |  1 |  1 | 100 |       -1 |\n",
    "| 100 |  1 |  1 |   2 |       -1 |\n",
    "| 100 |  0 |  1 |   2 |       -1 |\n",
    "|   2 |  0 |  0 | 100 |        1 |\n",
    "| 100 |  0 |  1 |   2 |        1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: (5 points) Define the optimization formulation without slack variables and without vector notation For instance, given:\n",
    "\n",
    "$$ W=\\begin{bmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ w_4 \\\\ \\end{bmatrix} $$\n",
    "\n",
    "and $ b $, the constraint for the first example without vector notation is:\n",
    "\n",
    "$$ 2w_1+1w_2+1w_3+100w_4+b\\le-1 $$\n",
    "\n",
    "Please note that also the optimization function should be defined without vector notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A1:\n",
    "\n",
    "Recall that that to maximize the length of the margin, we must minimize $ \\lVert W \\rVert = \\sqrt {w_1^2 + w_2^2 + w_3^2 + w_4^2} $. Since the square root function is monotonically increasing, we can instead minimize $ w_1^2 + w_2^2 + w_3^2 + w_4^2 $. The final form of the optimization function is then:\n",
    "\n",
    "$$ min(w_1^2 + w_2^2 + w_3^2 + w_4^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: (5 points) Please provide the formulation with the slack variables always without vector notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2:\n",
    "\n",
    "We also want to minimize the slack variables to ensure that we have the least number of outliers and that the outliers aren't too far into the wrong class. Since we don't want to favor the slack variables or $ W $ too much, we must add some scalar multiple $ \\lambda $ to ensure that neither is too favored:\n",
    "\n",
    "$$ min(w_1^2 + w_2^2 + w_3^2 + w_4^2 + \\lambda (s_1 + s_2 + s_3 + s_4 + s_5)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: (5 points) Is the formulation with the slack variables needed in this case? If Explain why in both the cases (yes or no)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A3:\n",
    "\n",
    "Slack variables are required in this case because we have outliers. Consider data-points 3 and 5. They both have the same exact feature values but different classes. One of them is an outlier. On the other hand, if the data is completely linearly separable, we would not need slack variables, as the slack variables exist to *push* the outliers back into their respective linear classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: (5 point) Assume you have only these linear separators:\n",
    "\n",
    "- $ H_1 = -2x_2 - x_3 + 2 $\n",
    "- $ H_2 = -2x_2 + 1 $\n",
    "- $ H_3 = 10x_1 + 10x_2 + 100x_3 + 100x_4 - 1000 $\n",
    "- $ H_4 = -4x_2 - x_3 + 3 $\n",
    "\n",
    "What is the best one according the dataset? Explain why the others are not the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A4:\n",
    "\n",
    "To determine how *good* a separating hyperplane is, we first check the constraints ($ W^{\\bf T}x + b \\ge 1, C(x) = 1 $ and $ W^{\\bf T}x + b \\le -1, C(x) = -1 $):\n",
    "\n",
    "| x index | C(x) | H_1 value | H_2 value | H_3 value | H_4 value |\n",
    "| -------:| ----:| ---------:| ---------:| ---------:| ---------:|\n",
    "|       1 |   -1 |        -1 |        -1 |      9130 |        -2 |\n",
    "|       2 |   -1 |        -1 |        -1 |       310 |        -2 |\n",
    "|       3 |   -1 |         1 |         1 |       300 |         2 |\n",
    "|       4 |    1 |         2 |         1 |      9020 |         3 |\n",
    "|       5 |    1 |         1 |         1 |       300 |         2 |\n",
    "\n",
    "It looks like $ H_1, H_2, H_4 $ all classify 4/5 correctly, whereas $ H_3 $ only classifies 2/5 correctly. The third row looks to be an outlier. To decide between $ H_1, H_2, H_4, $ we use the optimization function above:\n",
    "\n",
    "$$ \\begin{align}\n",
    "    O(W) & = w_1^2 + w_2^2 + w_3^2 + w_4^2 + \\lambda (s_1 + s_2 + s_3 + s_4 + s_5) \\\\\n",
    "    O(H_1) & = 5 + 2 \\lambda \\\\\n",
    "    O(H_2) & = 4 + 2 \\lambda \\\\\n",
    "    O(H_4) & = 17 + 3 \\lambda \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "So, $ H_2 $ appears to be the best classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: (5 points) Write a python code to\n",
    "\n",
    "- Plot in 2D the dataset\n",
    "- Find a way to obtain the definition (the expression) of the new 2 dimensions in the 2D plot (print them).\n",
    "- For each new dimension discuss the relevance of the original dimensions (x1, x2, x3, and x4).\n",
    "\n",
    "Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL0kN+v+iYGku3kNhMQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff62467d588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA (as a matrix) = [[-7.07101668e-01  1.20253646e-03 -3.60771375e-03  7.07101668e-01]\n",
      " [-4.54016445e-04  8.81675772e-01  4.71855296e-01  4.54016445e-04]]\n",
      "PCA_1 = -0.7071016681876731 * x_1 + 0.0012025364648884374 * x_2 + -0.003607713745579882 * x_3 + 0.7071016681876733 * x_4\n",
      "PCA_2 = -0.0004540164449234451 * x_1 + 0.8816757722223221 * x_2 + 0.4718552960541049 * x_3 + 0.0004540164449414741 * x_4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.decomposition as dc\n",
    "import matplotlib.pyplot as plt\n",
    "X = np.matrix([[2, 1, 1, 100], [100, 1, 1, 2], [100, 0, 1, 2], [2, 0, 0, 100], [100, 0, 1, 2]])\n",
    "y = [-1, -1, -1, 1, 1]\n",
    "\n",
    "# Compute the PCA\n",
    "pca = dc.PCA(2, random_state = 42)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot the PCA, with the first three rows (class = -1) red and the last two rows (class = 1) blue.\n",
    "plt.scatter(X_pca[:3,0], X_pca[:3,1], c = ['red'])\n",
    "plt.scatter(X_pca[3:,0], X_pca[3:,1], c = ['blue'])\n",
    "plt.show()\n",
    "\n",
    "# Get the components of the PCA, then print the result as a linear combination of x values\n",
    "print('PCA (as a matrix) =', pca.components_)\n",
    "print('PCA_1 =', pca.components_[0, 0], '* x_1 +', pca.components_[0, 1], '* x_2 +', pca.components_[0, 2], '* x_3 +', pca.components_[0, 3], '* x_4')\n",
    "print('PCA_2 =', pca.components_[1, 0], '* x_1 +', pca.components_[1, 1], '* x_2 +', pca.components_[1, 2], '* x_3 +', pca.components_[1, 3], '* x_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, $ PCA_1 $ depends mostly on $ x_1, x_4 $ (-0.707 and 0.707, respectively, larger coefficient magnitudes), and $ PCA_2 $ depends mostly on $ x_2 $ (0.882, largest coefficient magnitude) and secondarily on $ x_3 $ (0.472, second-largest coefficient magnitude). You can see why this might be the case, as $ x_1, x_4 $ vary much more than $ x_2, x_3 $. in the original dataset. Also, $ x_2 $ varies more than $ x_3 $. Even though $ x_2, x_3 $ have the same range of values, $ x_2 $ is more diverse (3 0's and 2 1's as opposed to 1 0 and 4 1's)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note: This is why we need to transform the data rows to std-dev = 1, so that no one feature dominates, as right now, feature 1 and 2 dominate because they have values of 100 for some points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
