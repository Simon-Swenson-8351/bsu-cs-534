{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 534: Spring 2018\n",
    "# Midterm Exam\n",
    "\n",
    "This exam has 7 questions for a total of 130 points.\n",
    "\n",
    "Name: Simon Swenson\n",
    "\n",
    "| Question | Points | Score |\n",
    "| --------:| ------:| -----:|\n",
    "|        1 |     30 |       |\n",
    "|        2 |     20 |       |\n",
    "|        3 |     10 |       |\n",
    "|        4 |     20 |       |\n",
    "|        5 |     15 |       |\n",
    "|        6 |     15 |       |\n",
    "|        7 |     20 |       |\n",
    "|   Total: |    130 |       |\n",
    "\n",
    "Be sure to show all work. Attach extra sheets as necessary. Collaboration is allowed for only 30 points. Please indicate your collaborator and where you collaborate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: (30 Points) Design a gradient descent algorithm to fit a logistic regression with the following training set (show the derivatives for each component, no vector notation allowed):\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "| --:| --:| --:| -:|\n",
    "|  0 |  1 |  0 | 1 |\n",
    "|  1 |  0 |  0 | 1 |\n",
    "|  0 |  0 |  1 | 1 |\n",
    "|  1 |  1 |  1 | 0 |\n",
    "|  1 |  1 |  0 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the logistic function is defined as:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{e^{wx + b}}{1 + e^{wx + b}} = \\frac{1}{1 + \\frac{1}{e^{wx + b}}}\n",
    "$$\n",
    "\n",
    "Recall the formulation for log likelihood:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    log L(w, b | x) &= \\frac {\\sum_{i = 1}^n y_i log(P(y_i | x_i, w, b)) + (1 - y_i) log(1 - P(y_i | x_i, w, b))} {N} \\\\\n",
    "                 &= \\frac {\\sum_{i = 1}^n y_i log(\\frac {1} {1 + \\frac {1} {e^{wx + b}}}) + (1 - y_i) log(1 - \\frac {1} {1 + \\frac {1} {e^{wx + b}}})} {N} \\\\\n",
    "                 &= \\frac {\\sum_{i = 1}^n y_i log(\\frac {1} {1 + \\frac {1} {e^{w_1x_1 + w_2x_2 + w_3x_3 + b}}}) + (1 - y_i) log(1 - \\frac {1} {1 + \\frac {1} {e^{w_1x_1 + w_2x_2 + w_3x_3 + b}}})} {N}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then the gradient will be a vector in 4-space with partials for each $ w_i $ and $ b $ term:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial log L} {\\partial w_1} &= \\sum_{i = 1}^n x_{i(1)} (y_i - f(x_i)) \\\\\n",
    "    \\frac{\\partial log L} {\\partial w_2} &= \\sum_{i = 1}^n x_{i(2)} (y_i - f(x_i)) \\\\\n",
    "    \\frac{\\partial log L} {\\partial w_3} &= \\sum_{i = 1}^n x_{i(3)} (y_i - f(x_i)) \\\\\n",
    "    \\frac{\\partial log L} {\\partial b} &= \\sum_{i = 1}^n y_i - f(x_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "(I don't know if you wanted us to compute the derivatives by hand, but I used Wolfram Alpha to help me out a bit with the calculus.)\n",
    "\n",
    "Note that we do not normalize the gradient.\n",
    "\n",
    "Then the algorithm is:\n",
    "\n",
    "```\n",
    "delta = infinity\n",
    "last-cost = -infinity\n",
    "while delta > stopping-delta:\n",
    "    dw1 = 0\n",
    "    dw2 = 0\n",
    "    dw3 = 0\n",
    "    db = 0\n",
    "    for each data-point in data:\n",
    "        # I use base-1 indexing to be consistent with the weight/x labels in the math above\n",
    "        dw1 += data-point[1] * (classes[data-point] - logistic(data-point, w1, w2, w3, b))\n",
    "        dw2 += data-point[2] * (classes[data-point] - logistic(data-point, w1, w2, w3, b))\n",
    "        dw3 += data-point[3] * (classes[data-point] - logistic(data-point, w1, w2, w3, b))\n",
    "        db += class[data-point] - logistic(data-point, w1, w2, w3, b)\n",
    "    # Since we are to maximize likelihood, let's add the gradient\n",
    "    w1 += dw1\n",
    "    w2 += dw2\n",
    "    w3 += dw3\n",
    "    b += db\n",
    "    delta = likelihood(data, classes, w1, w2, w3, b) - last-cost\n",
    "    last-cost = likelihood(data, classes, w1, w2, w3, b)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code\n",
    "import numpy as np\n",
    "import sys\n",
    "def logistic(data, weights, bias):\n",
    "    return 1/(1 + np.exp(-(data[0]*weights[0] + data[1]*weights[1] + data[2]*weights[2] + bias)))\n",
    "\n",
    "def likelihood(data, classes, weights, bias):\n",
    "    result = 1.0\n",
    "    for data_cur, class_cur in zip(data, classes):\n",
    "        logi = logistic(data_cur, weights, bias)\n",
    "        result *= logi ** class_cur + (1 - logi) ** (1 - class_cur)\n",
    "    return result\n",
    "    \n",
    "def gradient_descent(data, classes, weights, bias, speed = 0.01, stopping_delta = 0.01):\n",
    "    delta = sys.float_info.max\n",
    "    last_cost = sys.float_info.max\n",
    "    while delta > stopping_delta:\n",
    "        dw = [0] * 3\n",
    "        db = 0\n",
    "        for data_cur, class_cur in zip(data, classes):\n",
    "            diff = class_cur - logistic(data_cur, weights, bias)\n",
    "            dw[0] += data_cur[0] * diff\n",
    "            dw[1] += data_cur[1] * diff\n",
    "            dw[2] += data_cur[2] * diff\n",
    "            db += diff\n",
    "        weights[0] += dw[0] * speed\n",
    "        weights[1] += dw[1] * speed\n",
    "        weights[2] += dw[2] * speed\n",
    "        bias += db * speed\n",
    "        cost = -likelihood(data, classes, weights, bias)\n",
    "        delta = abs(cost - last_cost)\n",
    "        last_cost = cost\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: (20 points) Change the formulation of the logistic regression and its gradient descendent algorithm to handle the importance (IP) of the examples (no vector notation is allowed, show the algorithm).\n",
    "\n",
    "| x1 | x2 | x3 | y | IP |\n",
    "| --:| --:| --:| -:| --:|\n",
    "|  0 |  1 |  0 | 1 |  2 |\n",
    "|  1 |  0 |  0 | 1 |  2 |\n",
    "|  0 |  0 |  1 | 1 |  2 |\n",
    "|  1 |  1 |  1 | 0 |  3 |\n",
    "|  1 |  1 |  0 | 0 |  3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the likelihood function to:\n",
    "\n",
    "$$\n",
    "log L(w, b | x) = \\frac {\\sum_{i = 1}^n ip_i (y_i log(\\frac {1} {1 + \\frac {1} {e^{w_1x_1 + w_2x_2 + w_3x_3 + b}}}) + (1 - y_i) log(1 - \\frac {1} {1 + \\frac {1} {e^{w_1x_1 + w_2x_2 + w_3x_3 + b}}}))} {N}\n",
    "$$\n",
    "\n",
    "Note N must also be changed:\n",
    "\n",
    "$$\n",
    "N = \\sum_{i = 1}^n ip_i\n",
    "$$\n",
    "\n",
    "Then the gradient becomes:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial log L} {\\partial w_1} &= \\sum_{i = 1}^n ip_i x_{i(1)} (y_i - f(x_i)) \\\\\n",
    "    \\frac{\\partial log L} {\\partial w_2} &= \\sum_{i = 1}^n ip_i x_{i(2)} (y_i - f(x_i)) \\\\\n",
    "    \\frac{\\partial log L} {\\partial w_3} &= \\sum_{i = 1}^n ip_i x_{i(3)} (y_i - f(x_i)) \\\\\n",
    "    \\frac{\\partial log L} {\\partial b} &= \\sum_{i = 1}^n ip_i (y_i - f(x_i))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code\n",
    "import numpy as np\n",
    "import sys\n",
    "def logistic_imp(data, weights, bias):\n",
    "    return 1/(1 + np.exp(-(data[0]*weights[0] + data[1]*weights[1] + data[2]*weights[2] + bias)))\n",
    "\n",
    "def likelihood_imp(data, classes, importance, weights, bias):\n",
    "    result = 1.0\n",
    "    for data_cur, class_cur, importance_cur in zip(data, classes, importance):\n",
    "        logi = logistic_imp(data_cur, weights, bias)\n",
    "        result *= importance_cur * (logi ** class_cur + (1 - logi) ** (1 - class_cur))\n",
    "    return result\n",
    "    \n",
    "def gradient_descent_imp(data, classes, importance, weights, bias, speed = 0.01, stopping_delta = 0.01):\n",
    "    delta = sys.float_info.max\n",
    "    last_cost = sys.float_info.max\n",
    "    while delta > stopping_delta:\n",
    "        dw = [0] * 3\n",
    "        db = 0\n",
    "        for data_cur, class_cur, importance_cur in zip(data, classes, importance):\n",
    "            diff = importance_cur * (class_cur - logistic_imp(data_cur, weights, bias))\n",
    "            dw[0] += data_cur[0] * diff\n",
    "            dw[1] += data_cur[1] * diff\n",
    "            dw[2] += data_cur[2] * diff\n",
    "            db += diff\n",
    "        weights[0] += dw[0] * speed\n",
    "        weights[1] += dw[1] * speed\n",
    "        weights[2] += dw[2] * speed\n",
    "        bias += db * speed\n",
    "        cost = -likelihood_imp(data, classes, importance, weights, bias)\n",
    "        delta = abs(cost - last_cost)\n",
    "        last_cost = cost\n",
    "        #print(cost)\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I think I maybe generalized more than you wanted for Q1 and Q2. I have the code calculating each gradient coefficient. The example you gave just had the coefficients pre-calculated by hand, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: (10 points) Show the results for both points 1 and point 2 for 3 different initial points in input to the gradient descendent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 1, 1], [1, 1, 0]]\n",
    "classes = [1, 1, 1, 0, 0]\n",
    "importance = [2, 2, 2, 3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-6.6615549760803745, -6.6615549760803745, -2.421964436360076],\n",
       " 10.122710661461)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(data, classes, [0, 0, 0], 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-6.658291394397741, -6.658291394397741, -2.41657273234312],\n",
       " 10.117572435571061)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(data, classes, [3, 3, 3], -3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-6.781998426400366, -6.596651299776646, -3.882604932995159],\n",
       " 10.209943432929636)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(data, classes, [-1, 1, -2], 14, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-11.917666892123211, -11.917666892123211, -3.264227388327358],\n",
       " 17.822948536586665)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent_imp(data, classes, importance, [0, 0, 0], 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-11.916869544241836, -11.916869544241836, -3.3024333320435963],\n",
       " 17.822457344183352)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent_imp(data, classes, importance, [3, 3, 3], -3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-11.888144539940846, -11.888143752924751, -4.480750513872881],\n",
       " 17.79204647078796)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent_imp(data, classes, importance, [-1, 1, -2], 14, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4: (20 points) By using a binary classifier (logistic regression or SVM) please implement in python the 2 different procedure (one vs. one, one vs. other) to handle the problem of more than two different classes (use the iris dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import utils\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn import svm\n",
    "\n",
    "data = utils.shuffle(np.array(pd.read_csv('iris1.csv')), random_state = 42)\n",
    "X = data[:, :4]\n",
    "y = data[:, 4].flatten()\n",
    "\n",
    "le = pp.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "classoh = pp.OneHotEncoder()\n",
    "y = classoh.fit_transform(y.reshape(150, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5333333333333333\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# one-vs-others\n",
    "one_vs_others_classifiers = [svm.LinearSVC(), svm.LinearSVC(), svm.LinearSVC()]\n",
    "for i in range(3):\n",
    "    yy = y[:, i].toarray().flatten()\n",
    "    # Split between training and test\n",
    "    X_tr = X[:len(X) * 9 // 10, :]\n",
    "    X_te = X[len(X) * 9 // 10:, :]\n",
    "    y_tr = yy[:len(X) * 9 // 10]\n",
    "    y_te = yy[len(X) * 9 // 10:]\n",
    "    one_vs_others_classifiers[i].fit(X_tr, y_tr)\n",
    "    classes = one_vs_others_classifiers[i].predict(X_te)\n",
    "    accur = np.sum([1 if y_te_cur == class_cur else 0 for y_te_cur, class_cur in zip(y_te, classes)])/len(y_te)\n",
    "    print(accur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# one-vs-one\n",
    "one_vs_one_classifiers = {}\n",
    "for i in range(2):\n",
    "    for j in range(i + 1, 3):\n",
    "        XX = []\n",
    "        yy = []\n",
    "        for X_cur, y_cur in zip(X, y.toarray()):\n",
    "            if y_cur[i] == 1:\n",
    "                XX.append(X_cur)\n",
    "                yy.append(1)\n",
    "            elif y_cur[j] == 1:\n",
    "                XX.append(X_cur)\n",
    "                yy.append(0)\n",
    "        XX = np.array(XX)\n",
    "        yy = np.array(yy)\n",
    "        X_tr = XX[:len(XX) * 9 // 10, :]\n",
    "        X_te = XX[len(XX) * 9 // 10:, :]\n",
    "        y_tr = yy[:len(XX) * 9 // 10]\n",
    "        y_te = yy[len(XX) * 9 // 10:]\n",
    "        cur_classifier = svm.LinearSVC()\n",
    "        cur_classifier.fit(X_tr, y_tr)\n",
    "        classes = cur_classifier.predict(X_te)\n",
    "        accur = np.sum([1 if y_te_cur == class_cur else 0 for y_te_cur, class_cur in zip(y_te, classes)])/len(y_te)\n",
    "        print(accur)\n",
    "        one_vs_one_classifiers[(i, j)] = cur_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5: (15 points) Implement the NaÃ¯ve Bayes classifier, use the training set in point 1 and test it with the following dataset\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "| --:| --:| --:| -:|\n",
    "|  1 |  0 |  0 | 0 |\n",
    "|  1 |  0 |  1 | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification for [1 0 0] : (1, 0.08888888888888886)\n",
      "classification for [1 0 1] : (1, 0.04444444444444443)\n"
     ]
    }
   ],
   "source": [
    "X_tr = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 1, 1], [1, 1, 0]])\n",
    "y_tr = np.array([1, 1, 1, 0, 0])\n",
    "\n",
    "# For each class, for each feature, for each category of that feature,\n",
    "# we must calculate P(cat | class). Also need P(class)\n",
    "\n",
    "# Creates a new several-layers deep dictionary that matches class + feature index + feature\n",
    "# to a probability and class to a probability. Initializes all leaf nodes to 0.\n",
    "# The structure is:\n",
    "# feature_instances\n",
    "#     classes\n",
    "#         features\n",
    "#             feature category probability (likelihood), initialized to 0.\n",
    "# class_instances\n",
    "#     class probability (num class/total instances), initialized to 0.\n",
    "def empty_NBC(feature_domain, class_domain):\n",
    "    result = {'feature_instances': {}, 'class_instances': {}}\n",
    "    for class_domain_cur in class_domain:\n",
    "        result['feature_instances'][class_domain_cur] = {}\n",
    "        result['class_instances'][class_domain_cur] = 0\n",
    "        for feature_domain_idx in range(len(feature_domain)):\n",
    "            result['feature_instances'][class_domain_cur][feature_domain_idx] = {}\n",
    "            for feature in feature_domain[feature_domain_idx]:\n",
    "                result['feature_instances'][class_domain_cur][feature_domain_idx][feature] = 0\n",
    "    return result\n",
    "\n",
    "# Fills an assumed empty dictionary, first with the number of occurrances of each\n",
    "# situation, then normalizes/divides by the number of occurrances for the corresponding\n",
    "# class.\n",
    "def fill_NBC(NBC, X_tr, y_tr):\n",
    "    total = len(y_tr)\n",
    "    for X_cur, y_cur in zip(X_tr, y_tr):\n",
    "        # Count the number of occurrances of each class\n",
    "        NBC['class_instances'][y_cur] +=1\n",
    "        for feature_idx in range(len(X_cur)):\n",
    "            feature = X_cur[feature_idx]\n",
    "            # Count the number of occurrances of each situation\n",
    "            NBC['feature_instances'][y_cur][feature_idx][feature] += 1\n",
    "    \n",
    "    # Divide the number of each situation by the corresponding class, calculating likelihood\n",
    "    for i in NBC['feature_instances']:\n",
    "        for j in NBC['feature_instances'][i]:\n",
    "            for k in NBC['feature_instances'][i][j]:\n",
    "                # Divide by the number of instances of the corresponding class\n",
    "                NBC['feature_instances'][i][j][k] /= NBC['class_instances'][i]\n",
    "                \n",
    "    # P(c)\n",
    "    total = len(y_tr)\n",
    "    for i in NBC['class_instances']:\n",
    "        NBC['class_instances'][i] /= total\n",
    "                \n",
    "# Do the classification, comparing the output probabilities of the model for each class and\n",
    "# picking the one with the highest probability.\n",
    "def classify(NBC, X_te):\n",
    "    best_prediction = -1.0\n",
    "    best_prediction_class = ''\n",
    "    for cclass in NBC['class_instances']:\n",
    "        # Calculate the probability using Bayes's Theorem\n",
    "        cur_prediction = NBC['class_instances'][cclass]\n",
    "        for col_idx in range(len(X_te)):\n",
    "            col = X_te[col_idx]\n",
    "            cur_prediction *= NBC['feature_instances'][cclass][col_idx][col]\n",
    "        # Only keep the best probability/prediction\n",
    "        if cur_prediction > best_prediction:\n",
    "            best_prediction = cur_prediction\n",
    "            best_prediction_class = cclass\n",
    "    return best_prediction_class, best_prediction\n",
    "    \n",
    "NBC = empty_NBC([[0, 1], [0, 1], [0, 1]], [0, 1])\n",
    "fill_NBC(NBC, X_tr, y_tr)\n",
    "\n",
    "X_te = np.array([[1, 0, 0], [1, 0, 1]])\n",
    "y_te = np.array([0, 1])\n",
    "\n",
    "for point in X_te:\n",
    "    print('classification for', point, \":\", classify(NBC, point.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6: (15 points) Implement the K-nearest neighbor classifier with the same training and test of the point 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify_KNN(X_tr, y_tr, X_te, K):\n",
    "    class_distances = []\n",
    "    for X_tr_cur, y_tr_cur in zip(X_tr, y_tr):\n",
    "        distance = 0\n",
    "        # I use euclidian (l2) distance. You can easily do something else here.\n",
    "        for X_tr_feature, X_te_feature in zip(X_tr_cur, X_te):\n",
    "            distance += (X_te_feature - X_tr_feature) ** 2\n",
    "        distance = distance ** 0.5\n",
    "        # Building up a tuple that contains the distance calculation and the corresponding class\n",
    "        class_distances.append((y_tr_cur, distance))\n",
    "    # Sort the tuples by distance.\n",
    "    class_distances = sorted(class_distances, key = lambda cur: cur[1])\n",
    "    \n",
    "    # Find the consensus of various classes\n",
    "    consensus = {}\n",
    "    for i in range(min(len(class_distances), K)):\n",
    "        if not class_distances[i][0] in consensus:\n",
    "            consensus[class_distances[i][0]] = 1\n",
    "        else:\n",
    "            consensus[class_distances[i][0]] += 1\n",
    "            \n",
    "    # Find the highest consensus\n",
    "    best_key_num = 0\n",
    "    best_key = ''\n",
    "    for key in consensus:\n",
    "        if consensus[key] > best_key_num:\n",
    "            best_key_num = consensus[key]\n",
    "            best_key = key\n",
    "    return best_key, best_key_num\n",
    "    \n",
    "classify_KNN(X_tr, y_tr, X_te[0], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7: (20 points) Create examples to explain the property an the importance of the following kernels:\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Graph_kernel\n",
    "- https://en.wikipedia.org/wiki/String_kernel\n",
    "- https://en.wikipedia.org/wiki/Polynomial_kernel, in this case make a comparison with another procedure that in not based on kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you were tasked creating a system that will classify systems of roads into several categories (city, suburban, country, dirt, etc.), just given their intersections (the nodes) and distance to each intersection (the connection). These graphs are not bounded in any way. Can a road system have 100 roads and 200 connections? What about 1000 roads and 10,000 connections? It is not a well-defined feature set. However, we can use the graph kernel to compare these graphs anyway, even if they are completely different. Using the kernel process, we can create a feature set with length equal to the number of data points, where each feature is the graph kernel of a point in the data set and the current graph. This allows us to transform the data into something that is easier to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words and sentences, just like the graph above, contain an indefinate number of characters. It is possible to create a run-on sentence that goes on virtually forever. We cannot have a feature space with an indefinate number of features. In addition, sentences like, \"I love machine learning!\" and \"Machine learning is great!\" would be represented totally different in a naive approach, where each character is represented as a feature. Since the phrase \"machine learning\" appears in different positions in the sentences, the representation of both in that feature space doesn't encode that similarity at all. Thus, we turn to a kernel function to compare the strings. This allows us to create another feature space similar to above, where each feature is the string kernel applied to a particular point in the data set and the current point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a nonlinear dataset, the polynomial kernel can be very useful. It allows us to transform the data using combinations of the features like $ x_1^2, x_1x_2,etc $. Recall the distance transformation, which can similarly classify nonlinear data. The distance transformation converts each feature to be the distance of that point to another specific point in the training data. Thus, the number of features will be proportional to the number of data points in the training data. However, with a polynomial kernel, the number of features will be proportional to the number of original features (on the order of number-of-features^n, where n is the number of degrees). Thus, we may want to use one or the other depending on if the data set has few features and many data points or many features and few data points. In addition, testing may be needed to find out the best degree of the polynomial classifier. Either way, the transformations allow us to classify nonlinear data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
