{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS 3\n",
    "\n",
    "Simon Swenson\n",
    "\n",
    "Collaborated With: none\n",
    "\n",
    "CS 534\n",
    "\n",
    "Dr. Serra\n",
    "\n",
    "4/20/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import k_means\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import sys\n",
    "import queue\n",
    "\n",
    "data = pd.read_csv('iris1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. (12 points) Write in Python the AdaBoost and check the results with the binary version of the IRIS dataset (consider the most difficult separation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the difficult classification is between virginica and versicolor.\n",
    "data_bin = data.as_matrix()\n",
    "X = data_bin[:, :-1]\n",
    "y = data_bin[:, -1].reshape((len(data_bin), ))\n",
    "weights = [1/len(data_bin)]*len(data_bin)\n",
    "num_rounds = 20\n",
    "num_samples = 50\n",
    "classifiers = [None]*num_rounds\n",
    "errors = [1.0]*num_rounds\n",
    "\n",
    "random.seed(42)\n",
    "for i in range(num_rounds):\n",
    "    idx = np.array([])\n",
    "    X_cur = np.matrix([])\n",
    "    y_cur = np.array([])\n",
    "    while errors[i] > 0.5:\n",
    "        idx = []\n",
    "        X_cur = []\n",
    "        y_cur = []\n",
    "        # Build up the current sample lists\n",
    "        for j in range(num_samples):\n",
    "            rng = random.random()\n",
    "            # Find out which row we need by seeing when our accumulator passes the generated random number.\n",
    "            accum = 0.0\n",
    "            for k in range(len(weights)):\n",
    "                accum += weights[k]\n",
    "                if accum > rng:\n",
    "                    idx.append(k)\n",
    "                    X_cur.append(X[k, :])\n",
    "                    y_cur.append(y[k])\n",
    "                    break\n",
    "        idx = np.array(idx)\n",
    "        X_cur = np.matrix(X_cur)\n",
    "        y_cur = np.array(y_cur)\n",
    "        # Arbitrarily choose logistic regression. Can be any binary classifier, though.\n",
    "        classifiers[i] = LogisticRegression();\n",
    "        # This line was necessary in case one of our random samples only picked from one class (unlikely, but possible)\n",
    "        if len(np.unique(y_cur)) == 1:\n",
    "            errors[i] = 1\n",
    "            continue\n",
    "        classifiers[i].fit(X_cur, y_cur)\n",
    "        # There was a bit of a discussion on whether to calculate the error here based on the subset or the whole\n",
    "        # training set. I will use the whole training set for this part.\n",
    "        # I will also arbitrarily choose accuracy as the error function.\n",
    "        errors[i] = 1 - metrics.accuracy_score(y, classifiers[i].predict(X))\n",
    "    for idx_cur in idx:\n",
    "        weights[idx_cur] *= errors[i]/(1 - errors[i])\n",
    "    norm_factor = np.sum(weights)\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] /= norm_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "versicolor\n",
      "virginica\n"
     ]
    }
   ],
   "source": [
    "# Pass in a single row, X\n",
    "def classify_AdaBoost(X):\n",
    "    class_weights = {'virginica': 0.0, 'versicolor': 0.0, 'setosa': 0.0}\n",
    "    for i in range(num_rounds):\n",
    "        weight_cur = np.log((1 - errors[i])/errors[i])\n",
    "        prediction = classifiers[i].predict(np.matrix(X))[0]\n",
    "        class_weights[prediction] += weight_cur\n",
    "    max_key = ''\n",
    "    max_val = 0.0\n",
    "    for key in class_weights:\n",
    "        if class_weights[key] > max_val:\n",
    "            max_key = key\n",
    "            max_val = class_weights[key]\n",
    "    return max_key\n",
    "\n",
    "# Random garbage data to test\n",
    "print(classify_AdaBoost([0, 0, 0, 0]))\n",
    "print(classify_AdaBoost([10, 10, 10, 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. (13 points) Write in python DBSCAN and by using IRIS dataset compare the DBSCAN results with ones of K-Means (for K-means, you can use the Scikit implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_epsilon = 0.41\n",
    "dbscan_neighbors = 3\n",
    "k_means_clusters = 3\n",
    "\n",
    "X = data.as_matrix()[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'noise', 0, 0, 0, 0, 0, 0, 0, 'noise', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'noise', 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 'noise', 1, 'noise', 1, 1, 1, 'noise', 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 'noise', 1, 'noise', 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 'noise', 2, 2, 2, 2, 'noise', 'noise', 'noise', 'noise', 'noise', 2, 2, 2, 2, 'noise', 2, 2, 'noise', 'noise', 'noise', 2, 2, 'noise', 2, 2, 'noise', 2, 2, 2, 'noise', 'noise', 'noise', 2, 2, 'noise', 'noise', 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[0.94, 0.76, 0.66]\n",
      "0.7866666666666666\n"
     ]
    }
   ],
   "source": [
    "def neighbors(X, X_cur, epsilon):\n",
    "    result = []\n",
    "    for i in range(X.shape[0]):\n",
    "        X_other = X[i, :].reshape((X.shape[1], ))\n",
    "        if not np.all(np.equal(X_cur, X_other)) and np.linalg.norm(X_cur - X_other) <= epsilon:\n",
    "            result.append(i)\n",
    "    return set(result)\n",
    "\n",
    "def trim_classified(neighbor_list):\n",
    "    result = []\n",
    "    for neighbor in neighbor_list:\n",
    "        if dbscan_clusters[neighbor] == 'not-processed' or dbscan_clusters[neighbor] == 'noise':\n",
    "            result.append(neighbor)\n",
    "    return set(result)\n",
    "\n",
    "dbscan_clusters = ['not-processed']*X.shape[0]\n",
    "cur_cluster = 0\n",
    "for i in range(X.shape[0]):\n",
    "    if dbscan_clusters[i] != 'not-processed':\n",
    "        continue\n",
    "    cur_neighbors = neighbors(X, X[i, :].reshape((X.shape[1], )), dbscan_epsilon)\n",
    "    if len(cur_neighbors) < dbscan_neighbors:\n",
    "        dbscan_clusters[i] = 'noise'\n",
    "        continue\n",
    "        \n",
    "    dbscan_clusters[i] = cur_cluster\n",
    "    cur_neighbors = trim_classified(cur_neighbors)\n",
    "    \n",
    "    # We use a queue to process each point one-by-one (almost like a breadth-first search).\n",
    "    points_to_process = queue.Queue()\n",
    "    for cur_neighbor in cur_neighbors:\n",
    "        points_to_process.put(cur_neighbor)\n",
    "        dbscan_clusters[cur_neighbor] = 'in-process'\n",
    "        \n",
    "    while not points_to_process.empty():\n",
    "        point_to_process = points_to_process.get()\n",
    "        dbscan_clusters[point_to_process] = cur_cluster\n",
    "        cur_neighbors = neighbors(X, X[point_to_process, :].reshape((X.shape[1], )), dbscan_epsilon)\n",
    "        if len(cur_neighbors) >= dbscan_neighbors:\n",
    "            cur_neighbors = trim_classified(cur_neighbors)\n",
    "            \n",
    "            for cur_neighbor in cur_neighbors:\n",
    "                points_to_process.put(cur_neighbor)\n",
    "                dbscan_clusters[cur_neighbor] = 'in-process'\n",
    "    cur_cluster += 1\n",
    "print(dbscan_clusters)\n",
    "dbscan_cluster_accur = [0, 0, 0]\n",
    "for i in range(3):\n",
    "    unique, counts = np.unique(dbscan_clusters[i*50:(i+1)*50], return_counts = True)\n",
    "    occurrence_dict = dict(zip(unique, counts))\n",
    "    for key in occurrence_dict:\n",
    "        if occurrence_dict[key] > dbscan_cluster_accur[i]:\n",
    "            dbscan_cluster_accur[i] = occurrence_dict[key]\n",
    "    dbscan_cluster_accur[i] /= 50\n",
    "print(dbscan_cluster_accur)\n",
    "print(np.sum(dbscan_cluster_accur)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.96, 0.72]\n",
      "0.8933333333333332\n"
     ]
    }
   ],
   "source": [
    "k_means_clusters = k_means(X, 3, random_state = 42)\n",
    "k_means_cluster_accur = [0, 0, 0]\n",
    "for i in range(3):\n",
    "    unique, counts = np.unique(k_means_clusters[1][i*50:(i+1)*50], return_counts = True)\n",
    "    occurrence_dict = dict(zip(unique, counts))\n",
    "    for key in occurrence_dict:\n",
    "        if occurrence_dict[key] > k_means_cluster_accur[i]:\n",
    "            k_means_cluster_accur[i] = occurrence_dict[key]\n",
    "    k_means_cluster_accur[i] /= 50\n",
    "print(k_means_cluster_accur)\n",
    "print(np.sum(k_means_cluster_accur)/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, K-means performs better than DBSCAN. This is likely because versicolor and verginica roughly intermingle together, but the point at which they intermingle is roughly linear (as can be seen from the PCA). Thus, k-means can model it better. DBSCAN has a hard time separating the two classes, and the parameters need to be very fine-tuned to prevent DBSCAN from classifying both versicolor and verginica as the same class due to this intermingling. Thus, the clustering accuracy suffers as a result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
